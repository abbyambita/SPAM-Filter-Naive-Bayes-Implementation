{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-9-47c74d62b05f>, line 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-47c74d62b05f>\"\u001b[1;36m, line \u001b[1;32m103\u001b[0m\n\u001b[1;33m    inTrainRange = 1\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "import copy\n",
    "from email import message_from_binary_file, policy\n",
    "import email.charset\n",
    "import numpy as np\n",
    "\n",
    "email.charset.ALIASES.update({\n",
    "    'iso-8859-8-i': 'iso-8859-8',\n",
    "    'x-mac-cyrillic': 'mac-cyrillic',\n",
    "    'macintosh': 'mac-roman',\n",
    "    'windows-874': 'cp874',\n",
    "    'default': 'utf-8',\n",
    "    'x-unknown': 'utf-8',\n",
    "    '%charset': 'utf-8',\n",
    "    'latin_1': 'iso-8859-1',\n",
    "    'latin-1': 'iso-8859-1',\n",
    "    'latin_2': 'iso-8859-2',\n",
    "    'latin-2': 'iso-8859-2',\n",
    "    'latin_3': 'iso-8859-3',\n",
    "    'latin-3': 'iso-8859-3',\n",
    "    'latin_4': 'iso-8859-4',\n",
    "    'latin-4': 'iso-8859-4',\n",
    "    'latin_5': 'iso-8859-9',\n",
    "    'latin-5': 'iso-8859-9',\n",
    "    'latin_6': 'iso-8859-10',\n",
    "    'latin-6': 'iso-8859-10',\n",
    "    'latin_7': 'iso-8859-13',\n",
    "    'latin-7': 'iso-8859-13',\n",
    "    'latin_8': 'iso-8859-14',\n",
    "    'latin-8': 'iso-8859-14',\n",
    "    'latin_9': 'iso-8859-15',\n",
    "    'latin-9': 'iso-8859-15',\n",
    "    'latin_10':'iso-8859-16',\n",
    "    'latin-10':'iso-8859-16',\n",
    "    'cp949':   'ks_c_5601-1987',\n",
    "    'euc_jp':  'euc-jp',\n",
    "    'euc_kr':  'euc-kr',\n",
    "    'ascii':   'us-ascii',\n",
    "})\n",
    "\n",
    "def extract_body(msg, depth=0):\n",
    "    body = []\n",
    "    if msg.is_multipart():\n",
    "        main_content = None\n",
    "        for part in msg.get_payload():\n",
    "            is_txt = part.get_content_type() == 'text/plain'\n",
    "            if not main_content or is_txt:\n",
    "                main_content = extract_body(part)\n",
    "            if is_txt:\n",
    "                break\n",
    "        if main_content:\n",
    "            body.extend(main_content)\n",
    "    elif msg.get_content_type().startswith(\"text/\"):\n",
    "        charset = msg.get_param('charset', 'utf-8').lower()\n",
    "        charset = email.charset.ALIASES.get(charset, charset)\n",
    "        msg.set_param('charset', charset)\n",
    "        try:\n",
    "            body.append(msg.get_content())\n",
    "        except AssertionError as e:\n",
    "            print('Parsing failed.    ')\n",
    "            print(e)\n",
    "        except LookupError:\n",
    "            msg.set_param('charset', 'utf-8')\n",
    "            body.append('=== <UNKOWN ENCODING POSSIBLY SPAM> ===')\n",
    "            body.append(msg.get_content())\n",
    "    return body\n",
    "\n",
    "\n",
    "email_labels = dict()\n",
    "train_spam = []\n",
    "train_ham = []\n",
    "test_spam = []\n",
    "test_ham = []\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "def toString(string):    \n",
    "    try:\n",
    "        return string.decode(\"utf-8\")\n",
    "    except ValueError:\n",
    "        return string\n",
    "\n",
    "def read_files():\n",
    "    start_time = time.time()\n",
    "    dir = 'dataset-cut\\data'\n",
    "    labelFile = 'dataset-cut\\labels'\n",
    "    \n",
    "    with open(labelFile, 'r') as label_file:\n",
    "        for cnt, line in enumerate(label_file):\n",
    "            line = line.strip()\n",
    "            line = re.sub('[.]', '', line).split(\" \")\n",
    "            email_labels[\"dataset-cut\"+line[1]] = line[0]\n",
    "\n",
    "    all_words = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        inTrainRange = 1\n",
    "        for name in files:\n",
    "            path = os.path.join(root, name).replace(\"\\\\\", r\"/\")\n",
    "            if os.path.isfile(path):\n",
    "                folder = root.split(\"\\\\\")\n",
    "                classify = \"\"\n",
    "                if path in email_labels:\n",
    "                    classify = email_labels[path]\n",
    "                    \n",
    "                if(len(folder)>2):\n",
    "                    inTrainRange = int(folder[2]) < 71\n",
    "\n",
    "                msg = message_from_binary_file(open(path, mode=\"rb\"), policy=policy.default)\n",
    "                body = '\\n\\n'.join(extract_body(msg))\n",
    "                word_regex = \"[a-zA-Z]+\"\n",
    "                words_per_file = re.findall(word_regex, body)\n",
    "                for k in range(len(words_per_file)):\n",
    "                    words_per_file[k] = words_per_file[k].lower()\n",
    "                    words_per_file[k] = words_per_file[k].replace(\"_\", \"\")\n",
    "                    words_per_file[k] = re.sub(\"[0-9]+\", \"\", words_per_file[k])\n",
    "                words = [word for word in words_per_file if len(word) >= 4 and word.isalpha()]\n",
    "\n",
    "                if(inTrainRange):\n",
    "                    if(classify == \"spam\"):\n",
    "                        train_spam.append(words)\n",
    "                    else:\n",
    "                        train_ham.append(words)\n",
    "                else:\n",
    "                    if(classify == \"spam\"):\n",
    "                        test_spam.append(words)\n",
    "                    else:\n",
    "                        test_spam.append(words)\n",
    "\n",
    "                        \n",
    "                if(inTrainRange):\n",
    "                    all_words += words\n",
    "\n",
    "\t\t\t\t# with open(path, \"rb\") as text_file:\n",
    "\t\t\t\t# \twords = []\n",
    "\t\t\t\t# \tfor line in text_file:\n",
    "\t\t\t\t# \t\tword_regex = re.compile(b\"[a-zA-Z]+\")\n",
    "\t\t\t\t# \t\treceived_regex = re.compile(b\"^Received:\")\n",
    "\t\t\t\t# \t\tfrom_regex = re.compile(b\"^From:\")\n",
    "\t\t\t\t# \t\tto_regex = re.compile(b\"^To:\")\n",
    "\t\t\t\t# \t\tsender_regex = re.compile(b\"^Sender:\")\n",
    "\t\t\t\t# \t\tcontent_regex = re.compile(b\"^Content-\\S+\")\n",
    "\t\t\t\t# \t\tx_regex = re.compile(b\"^X-\\S+:\")\n",
    "\t\t\t\t# \t\tpath_regex = re.compile(b\"^Path\")\n",
    "\t\t\t\t# \t\tchar_regex = re.compile(b\"^<\")\n",
    "\n",
    "\t\t\t\t# \t\tline = line.rstrip()\n",
    "\t\t\t\t# \t\tif re.search(received_regex, line) or re.search(from_regex, line) or re.search(to_regex, line) or re.search(content_regex, line) or re.search(x_regex, line) or re.search(path_regex, line) or re.search(sender_regex, line) or re.search(char_regex, line):\n",
    "\t\t\t\t# \t\t\tcontinue\n",
    "\n",
    "\t\t\t\t# \t\twords_per_file = re.findall(word_regex, line)\n",
    "\t\t\t\t# \t\twords_per_file = [toString(word) for word in words_per_file]\n",
    "\t\t\t\t# \t\tfor k in range(len(words_per_file)):\n",
    "\t\t\t\t# \t\t\twords_per_file[k] = words_per_file[k].lower()\n",
    "\t\t\t\t# \t\t\twords_per_file[k] = words_per_file[k].replace(\"_\", \"\")\n",
    "\t\t\t\t# \t\t\twords_per_file[k] = re.sub(\"[0-9]+\", \"\", words_per_file[k])\n",
    "\t\t\t\t# \t\twords += [word for word in words_per_file if len(word) >= 4 and word.isalpha()]\n",
    "\n",
    "\t\t\t\t# \tif(inTrainRange):\n",
    "\t\t\t\t# \t\tif(classify == \"spam\"):\n",
    "\t\t\t\t# \t\t\ttrain_spam.append(words)\n",
    "\t\t\t\t# \t\telse:\n",
    "\t\t\t\t# \t\t\ttrain_ham.append(words)\n",
    "\t\t\t\t# \telse:\n",
    "\t\t\t\t# \t\tif(classify == \"spam\"):\n",
    "\t\t\t\t# \t\t\ttest_spam.append(words)\n",
    "\t\t\t\t# \t\telse:\n",
    "\t\t\t\t# \t\t\ttest_spam.append(words)\n",
    "\n",
    "\n",
    "\t\t\t\t# \tif(inTrainRange):\n",
    "\t\t\t\t# \t\tall_words += words\n",
    "\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "\t\n",
    "    dictionary = Counter(all_words)\n",
    "    dictionary = {k:dictionary[k] for k in dictionary if dictionary[k] > 15}\n",
    "    print(str(len(dictionary)))\n",
    "    # print(dictionary)\n",
    "\n",
    "    return dictionary, train_spam, train_ham, test_spam, test_ham\n",
    "\n",
    "def merge_datasets():\n",
    "    #merge ham and spam trainsets and testsets\n",
    "    train_set = train_spam + train_ham\n",
    "\ttest_set = test_spam + test_ham\n",
    "\t\n",
    "\t#classify 1(spam) and 0(ham) for the trainset\n",
    "\ttrain_labels = [1] * len(train_spam)\n",
    "\ttrain_labels.extend([0] * len(train_ham))\n",
    "\n",
    "\t#classify 1(spam) and 0(ham) for the testset\n",
    "\ttest_labels = [1] * len(test_spam)\n",
    "\ttest_labels.extend([0] * len(test_spam))\n",
    "\treturn train_set, test_set, train_labels, test_labels\n",
    "\n",
    "\n",
    "def getFrequency():\n",
    "\tspam_frequency = len(train_spam)\n",
    "\tham_frequency = len(train_ham)\n",
    "\t\n",
    "\tspam_probability = spam_frequency / (spam_frequency + ham_frequency)\n",
    "\tham_probability = ham_frequency / (spam_frequency + ham_frequency)\n",
    "\n",
    "\tprint(\"F(SPAM): \" + str(spam_frequency))\n",
    "\tprint(\"F(HAM)\" + str(ham_frequency))\n",
    "\tprint(\"P(SPAM)\" + str(spam_probability))\n",
    "\tprint(\"P(HAM)\" + str(ham_probability))\n",
    "\n",
    "\treturn spam_frequency, ham_frequency, spam_probability,ham_probability\n",
    "\n",
    "def getFeatureMatrix(dictionary):\n",
    "\tspam_feature_matrix = []\n",
    "\tfor each_file in train_spam:\n",
    "\t\tfeature_vector = [0] * len(dictionary)\n",
    "\t\tfor each_word in each_file:\n",
    "\t\t\tfor d,dicword in enumerate(dictionary.keys()):\n",
    "\t\t\t\tif each_word == dicword:\n",
    "\t\t\t\t\tfeature_vector[d] = 1\n",
    "\t\tspam_feature_matrix.append(feature_vector)\n",
    "\n",
    "\n",
    "\tham_feature_matrix = []\n",
    "\tfor each_file in train_ham:\n",
    "\t\tfeature_vector = [0] * len(dictionary)\n",
    "\t\tfor each_word in each_file:\n",
    "\t\t\tfor d,dicword in enumerate(dictionary.keys()):\n",
    "\t\t\t\tif each_word == dicword:\n",
    "\t\t\t\t\tfeature_vector[d] = 1\n",
    "\t\tham_feature_matrix.append(feature_vector)\t\n",
    "\n",
    "\treturn spam_feature_matrix, ham_feature_matrix\n",
    "\n",
    "def main():\n",
    "\tdictionary, train_spam, train_ham, test_spam, test_ham = read_files()\n",
    "\ttrain_set, test_set, train_labels, test_labels = merge_datasets()\n",
    "\tspam_feature_matrix, ham_feature_matrix = getFeatureMatrix(dictionary)\n",
    "\tspam_frequency, ham_frequency, spam_probability,ham_probability = getFrequency()\n",
    "\n",
    "\tdistribution = len(train_set)\n",
    "\n",
    "\ta = np.array(spam_feature_matrix)\n",
    "    print(a)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
